{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["!pip install transformers -U\n","!pip install langdetect"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","from torch import optim\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import random\n","import json\n","from tqdm import tqdm\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n","from torchtext.data.metrics import bleu_score\n","from langdetect import detect\n","import spacy\n","\n","spacy_nlp = spacy.load(\"en_core_web_sm\")\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# config\n","config = {\n","    \"end_token\" : \"</s>\",\n","    \"start_ans\" : \"<extra_id_1>\",\n","    \"end_ans\" : \"<extra_id_1>\",\n","    \"batch_size\" : 8,\n","    \"max_len_context\" : 512,\n","    \"max_len_question\" : 20,\n","    \"task\" : \"ask_question: \",\n","    \"epoch\" : 2,\n","    \"path_model\" : \"t5_question_generation.pth\",\n","    \"learning_rate\" : 5e-4,\n","    \"schedule_rate\" : 0.83,\n","    \"period_decay\" : 3,\n","    \"accumulation_step\" : 32\n","}\n","\n","config_ask_question = {\n","    \"early_stopping\": True,\n","    \"max_length\": 20,\n","    \"min_length\" : 3,\n","    \"num_beams\": 4,\n","    \"prefix\": \"ask_question: \"\n","}\n","\n","configT5_model = T5Config.from_pretrained('t5-base')\n","configT5_model.task_specific_params['ask_question'] = config_ask_question"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#Tokenizer\n","class Tokenizer:\n","    def __init__(self,):\n","        self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","    \n","    def encode_context(self, context, start_ans, end_ans):\n","        \"\"\"\n","            this functin is used to encode the context with highlighting the positions of answer\n","            format of input context : \"aks_question : context <start_ans> answer <end_answer> context </s>\"\n","            \n","            return dict{input_ids, attention_mask, token_type_ids}\n","        \"\"\"\n","        before_ans = context[:start_ans]\n","        ans = context[start_ans:end_ans]\n","        after_ans = context[end_ans:]\n","        input = config['task'] + before_ans + config['start_ans'] + ans + config['end_ans'] + after_ans + \" \" + config['end_token']\n","        output = self.tokenizer.encode_plus(input, max_length = config['max_len_context'], pad_to_max_length=True)\n","        return output\n","    \n","    def encode_question(self, question):\n","        \"\"\"\n","            this function is used to encode the question\n","            format of input question : \"question </s>\"\n","            output : list of input_ids\n","        \"\"\"\n","        input = question + \" \" + config['end_token']\n","        output = self.tokenizer.encode_plus(input, max_length=config['max_len_question'], pad_to_max_length=True)\n","        return output"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# dataset\n","class QDataset(Dataset):\n","    \n","    def __init__(self, filename, tokenizer):\n","        self.data = pd.read_csv(filename)\n","        self.tokenizer = tokenizer\n","        self.shuffle()\n","        \n","    def shuffle(self):\n","        self.data = self.data.sample(frac=1)\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        item = self.data.iloc[idx]\n","        context, question, start_ans, end_ans = item['context'], item['question'], item['start_ans'], item['end_answer']\n","        input_dict, output_dict = self.tokenizer.encode_context(context, start_ans, end_ans), self.tokenizer.encode_question(question)\n","        return input_dict['input_ids'], input_dict['attention_mask'], output_dict['input_ids'], output_dict['attention_mask']"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# model\n","class QGModel(nn.Module):\n","    def __init__(self, configT5):\n","        super(QGModel, self).__init__()\n","        self.t5_model = T5ForConditionalGeneration.from_pretrained('t5-base', config=configT5)\n","        \n","    def forward(self, input_ids_ctx, attention_mask_ctx, input_ids_qt = None, attention_mask_qt = None):\n","        output = self.t5_model(input_ids=input_ids_ctx, attention_mask=attention_mask_ctx, \n","                               decoder_attention_mask=attention_mask_qt, lm_labels=input_ids_qt)\n","        return output\n","    \n","    def predict(self, intput_ids_ctx, attention_mask=None):\n","        output = self.t5_model.generate(intput_ids_ctx, attention_mask)\n","        return output"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def evaluate(model, data_loader, tokenizer, device):\n","    predictions = []\n","    score = 0\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for _, batch in enumerate(tqdm(data_loader)):\n","            input_ids_ctx = torch.stack(batch[0], dim=1).to(device)\n","            input_ids_qt = torch.stack(batch[2], dim=1).to(device)\n","            output = model.predict(input_ids_ctx)\n","            \n","            for k in range(output.shape[0]):\n","                ground_truth = tokenizer.tokenizer.decode(input_ids_qt[k].tolist())\n","                predicted_question = tokenizer.tokenizer.decode(output[k].tolist())\n","                predictions.append((ground_truth, predicted_question))\n","    return score, predictions"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def trainEpoch(model, optimizer, train_data_loader, val_data_loader, tokenizer, device):\n","    model.train()\n","    optimizer.zero_grad()\n","    losses = []\n","    for step, batch in enumerate(tqdm(train_data_loader)):\n","        input_ids_ctx = torch.stack(batch[0], dim=1).to(device)\n","        attention_mask_ctx = torch.stack(batch[1], dim=1).to(device)\n","        input_ids_qt = torch.stack(batch[2], dim=1).to(device)\n","        attention_mask_qt = torch.stack(batch[3], dim=1).to(device)\n","        \n","        loss, _, _, _ = model(input_ids_ctx, attention_mask_ctx, input_ids_qt=input_ids_qt, attention_mask_qt=attention_mask_qt)\n","        loss /= config['accumulation_step']\n","        loss.backward()\n","        \n","        if (step + 1) % config['accumulation_step'] == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            losses.append(loss.item())\n","    \n","    score, predictions = evaluate(model, val_data_loader, tokenizer, device)\n","    \n","    return losses, predictions"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# global training\n","def globalTraining(model, optimizer, train_dataset, val_dataset, tokenizer, device):\n","    model.to(device)\n","    for epoch in range(config['epoch']):\n","        train_data_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n","        val_data_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n","        losses, prediction = trainEpoch(model, optimizer, train_data_loader, val_data_loader, tokenizer, device)\n","        \n","        #print information\n","        print(\"Epoch {}/{}, loss = {} ----> {}\".format(epoch, config['epoch'], losses[0], losses[-1]))\n","        \n","        #save model\n","        checkpoint = {\n","            \"model_state_dict\" : model.state_dict(),\n","            \"optimizer_sate_dict\" : optimizer.state_dict()\n","        }\n","        torch.save(checkpoint, config['path_model'])\n","        with open(\"losses{}.json\".format(epoch + 1), \"w\") as file:\n","            json.dump(losses, file)\n","        with open(\"prediction{}.json\".format(epoch + 1), \"w\") as file:\n","            json.dump(prediction, file)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Training"]},{"metadata":{"trusted":true},"cell_type":"code","source":["tokenizer = Tokenizer()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["dataset = QDataset(\"/kaggle/input/squad20csv/squad2.0.csv\", tokenizer)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_dataset, val_dataset = random_split(dataset, [80000, 6562])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["loader = DataLoader(train_dataset, batch_size=8)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model = QGModel(configT5_model)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["optimizer = optim.Adam(model.parameters())\n","scheduler = optim.lr_scheduler.StepLR(optimizer, 3, config['schedule_rate'])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["globalTraining(model, optimizer, train_dataset, val_dataset, tokenizer, device)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Generate Question according to a context"]},{"metadata":{"trusted":true},"cell_type":"code","source":["tokenizer = Tokenizer()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model = QGModel(configT5_model).to(device)\n","state_dict = torch.load('/kaggle/input/questiongt5/t5_question_generation.pth', map_location=device)\n","model.load_state_dict(state_dict['model_state_dict'])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def generate(model, tokenizer, context, device, list_dict_answers = None):\n","    # check the length of context\n","    if len(tokenizer.tokenizer.tokenize(context)) >= config['max_len_context'] - 4:\n","        raise ValueError(\"context too long\")\n","    \n","    # check whether the context is in english\n","    lang = detect(context)\n","    if lang != 'en':\n","        raise ValueError('context should be in english')\n","    \n","    # define inputs of the model\n","    input_ids = []\n","    \n","    # preprocessing\n","    # name entity extraction\n","    ner = spacy_nlp(context)\n","    batch_size = len(ner.ents)\n","    answers = []\n","    for ent in ner.ents:\n","        temp_dict = tokenizer.encode_context(context, ent.start_char, ent.end_char)\n","        input_ids.append(temp_dict['input_ids'])\n","        answers.append(ent.text)\n","    \n","    if list_dict_answers is not None:\n","        for item in list_dict_answers:\n","            start_ans, end_ans = item['start_ans'], item['end_ans']\n","            answer = context[start_ans:end_ans]\n","            tem_dict = tokenizer.encode_context(context, start_ans, end_ans)\n","            input_ids.append(temp_dict['input_ids'])\n","            answers.append(answer)\n","            \n","            batch_size += 1\n","\n","    input_ids = torch.LongTensor(input_ids).to(device)\n","    \n","    #predict question\n","    results = []\n","    for k in range(batch_size):\n","        predicted_question = model.predict(input_ids[k].unsqueeze(0).to(device)).squeeze(0)\n","        results.append(\n","            {\n","                \"question\" : tokenizer.tokenizer.decode(predicted_question.tolist()),\n","                \"answer\" : answers[k]\n","            }\n","        )\n","    \n","    return results"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["context = \"The study of chemical kinetics concerns the second and third questions—that is, the rate at which a reaction yields products and the molecular-scale means by which a reaction occurs. This chapter examines the factors that influence the rates of chemical reactions, the mechanisms by which reactions proceed, and the quantitative techniques used to describe the rates at which reactions occur.\""],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["generate(model, tokenizer, context, device)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}