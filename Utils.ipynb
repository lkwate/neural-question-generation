{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json\n",
    "import re \n",
    "import unicodedata\n",
    "\n",
    "## import about transformer model\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, LayerNorm, TransformerDecoderLayer, TransformerDecoder, Transformer\n",
    "import math \n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition of constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = {\n",
    "    'd_model_complex' : 800,     ## dimension of complex model's features\n",
    "    'd_model_simple' : 768,  ## dimension of simple model's features\n",
    "    'd_inp_complex' : 800,  ## dimension of linear projection of query, key, and value vector : complex model\n",
    "    'd_inp_simple' : 768,  ## dimension of linear projection of query, key, and value vector : simple model\n",
    "    'nhead' : 8,   ## number of head in multihead attention\n",
    "    'nlayer' : 6,  ## number of layer in encoder and decoder\n",
    "    'vocab_size' : 30522,  ## vocabulary size\n",
    "    'ner_size' : 37, ## size of vocabulary of ner tag\n",
    "    'pos_size' : 107, ## size of vocabulary of pos tag\n",
    "    'd_pos' : 16,  ## dimension of embedding representation of pos tagging\n",
    "    'd_ner' : 16,  ## dimension of embedding representation of ner tagging\n",
    "    'd_emb' : 768, ## dimension of word embeddings provide by bertModel-base-uncased\n",
    "    'start_answer_token' : \"[unused0]\",  ## token follow by the answer spanned in the context \n",
    "    'end_answer_token' : \"[unused1]\",  ## the end token of the answer spanned in the context\n",
    "    'pad' : \"[PAD]\",  ## pad token\n",
    "    'cls' : \"[CLS]\",  ## cls token, begin token of sequence\n",
    "    'sep' : \"[SEP]\",  ## separate token\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package to import \n",
    "'''\n",
    "torch\n",
    "\n",
    "'''\n",
    "class BertEmb():\n",
    "    def __init__(self, path_to_tokenizer = None, path_to_model = None):\n",
    "        \n",
    "        if path_to_tokenizer:\n",
    "            try :\n",
    "                self.tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', path_to_tokenizer)\n",
    "            except : \n",
    "                print(\"Error of loading tokenizer from local file\")\n",
    "        else :\n",
    "            self.tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "        \n",
    "        if path_to_model: \n",
    "            try:\n",
    "                self.bertModel = torch.hub.load('huggingface/pytorch-transformers', 'model', path_to_model)\n",
    "            except: \n",
    "                print(\"Error of loading Bertmodel from local file\")\n",
    "        else :\n",
    "            self.bertModel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "            \n",
    "    def get_id_tokens(self, sentence):\n",
    "        \"\"\"\n",
    "            encode a given sentence by given its corresponding ids in dictionary\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(sentence, add_special_tokens = True)\n",
    "    \n",
    "    def tokenize(self, sentence): \n",
    "        \"\"\"\n",
    "            tokenization of a given sentence\n",
    "        \"\"\"\n",
    "        return self.tokenizer.tokenize(sentence)\n",
    "    \n",
    "    def insert_answers_indices(self, sentence, start_answer, end_answer): \n",
    "        \"\"\"\n",
    "            insert the answers boundaries id token in a sequence\n",
    "        \"\"\"\n",
    "        before_answer = self.tokenizer.encode(sentence[:start_answer])\n",
    "        answer = self.tokenizer.encode(sentence[start_answer:end_answer])\n",
    "        after_answer = self.tokenizer.encode(sentence[end_answer:])\n",
    "        result = [101]\n",
    "        result += before_answer + [1] + answer + [2] + after_answer + [102]\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def padding_sequence(self, previous_indexed_tokens): \n",
    "        \"\"\"\n",
    "            assumptions : within this function we consider len(previous_indexed_tokens) < max_input_length\n",
    "            padding a given sequence by adding the index of token '[PAD]' in dictionary \n",
    "            outputs : indexed_tokens, len(previous_indexed_tokens)\n",
    "        \"\"\"\n",
    "        previous_length = len(previous_indexed_tokens)\n",
    "        padding_length = self.max_input_length - previous_length\n",
    "        \n",
    "        # padding\n",
    "        indexed_tokens = previous_indexed_tokens + [0] * padding_length\n",
    "        \n",
    "        return indexed_tokens, previous_length\n",
    "    '''\n",
    "    \n",
    "    def get_word_emb(self, sentence, start_answer = None, end_answer = None):\n",
    "        \"\"\"\n",
    "            retreiving of word embeddings of the tokens of a given sentence by processing\n",
    "            sentence through BERTModel\n",
    "        \"\"\"\n",
    "        if start_answer and end_answer: \n",
    "            indexed_tokens = self.insert_answers_indices(sentence, start_answer, end_answer)\n",
    "        else :\n",
    "            indexed_tokens = self.get_id_tokens(sentence)\n",
    "        \"\"\"\n",
    "        #padding sequence if it's necessary\n",
    "        if len(indexed_tokens) < self.max_input_length:\n",
    "            indexed_tokens, index_padding = self.padding_sequence(indexed_tokens)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        return self.get_word_emb_by_indices(indexed_tokens)\n",
    "\n",
    "    def get_word_emb_by_indices(self, indexed_tokens): \n",
    "        \n",
    "        \n",
    "        segments_tensor = torch.tensor([[1] * len(indexed_tokens)])\n",
    "        indexed_tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "        #due to the fact that bertModel cannot take over 512 token, it's necessary to breakdown\n",
    "        #segments_tensor and indexed_tokens into minibatch of size 500\n",
    "        length = len(indexed_tokens)  # length of input\n",
    "        batch_size = 500  # batch size \n",
    "        nbatch = length // batch_size  # number of minibatch \n",
    "        remain = length != batch_size * nbatch\n",
    "        \n",
    "        output = []\n",
    "        for i in range(nbatch): \n",
    "            batch_indexed_tokens_tensor = indexed_tokens_tensor[:, i * batch_size : (i + 1) * batch_size]\n",
    "            batch_segments_tensor = segments_tensor[:, i * batch_size : (i + 1) * batch_size]\n",
    "            # process segments_tensor and indexed_tokens through bertModel\n",
    "            with torch.no_grad(): \n",
    "                encoders_layers, _ = self.bertModel(batch_indexed_tokens_tensor, token_type_ids = batch_segments_tensor)\n",
    "                encoders_layers = encoders_layers.squeeze(0)\n",
    "                \n",
    "            output.append(encoders_layers)\n",
    "        \n",
    "        if remain :\n",
    "            batch_indexed_tokens_tensor = indexed_tokens_tensor[:, nbatch * batch_size : length]\n",
    "            batch_segments_tensor = segments_tensor[:, nbatch * batch_size : length]\n",
    "            with torch.no_grad(): \n",
    "                encoders_layers, _ = self.bertModel(batch_indexed_tokens_tensor, token_type_ids = batch_segments_tensor)\n",
    "                encoders_layers = encoders_layers.squeeze(0)\n",
    "            output.append(encoders_layers)\n",
    "            \n",
    "        #concatenate (along the dimension 0 ) the list of output tensor \n",
    "        encoders_layers = torch.cat(output, dim = 0)\n",
    "        ## the result have the shape : (length_tokens, 768)\n",
    "        encoders_layers.unsqueeze(1)\n",
    "        return encoders_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocabulary\n",
    "the vocabulary will be the vacab provide by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(): \n",
    "    \n",
    "    def __init__(self, path_to_tag_json, path_to_word_vocab): \n",
    "        self.ner_ids = {}\n",
    "        self.ids_ner = {}\n",
    "        self.pos_ids = {}\n",
    "        self.ids_pos = {}\n",
    "        self.word_ids = {}\n",
    "        self.ids_word = {}\n",
    "        \n",
    "        ## process data of tag (Ner + Pos)\n",
    "        with open(path_to_tag_json) as json_file: \n",
    "            data = json.load(json_file)\n",
    "            pos_data = data['pos']\n",
    "            ner_data = data['ner']\n",
    "            \n",
    "            self.pos_ids['O'] = 0\n",
    "            self.ids_pos[0] = 'O'\n",
    "            index = 1\n",
    "            #process data of tag\n",
    "            for pos in pos_data['tags']: \n",
    "                self.pos_ids['B-' + pos] = index\n",
    "                self.ids_pos[index] = 'B-' + pos\n",
    "                index += 1 \n",
    "                self.pos_ids['I-' + pos] = index \n",
    "                self.ids_pos[index] = 'I-' + pos\n",
    "                index += 1\n",
    "            \n",
    "            self.ner_ids['O'] = 0\n",
    "            self.ids_ner[0] = 'O'\n",
    "            index = 1\n",
    "            #process data of ner \n",
    "            for ner in ner_data['tags']: \n",
    "                self.ner_ids['B-' + ner] = index \n",
    "                self.ids_ner[index] = 'B-' + ner\n",
    "                index += 1 \n",
    "                self.ner_ids['I-' + ner] = index\n",
    "                self.ids_ner[index] = 'I-' + ner\n",
    "                index += 1\n",
    "        \n",
    "        ## building of word dictionary\n",
    "        index = 0\n",
    "        with open(path_to_word_vocab) as file: \n",
    "            for line in file: \n",
    "                line = line.strip()\n",
    "                self.word_ids[line] = index \n",
    "                self.ids_word[index] = line \n",
    "                index += 1\n",
    "    def word_by_id(self, id): \n",
    "        return self.ids_word[id]\n",
    "        \n",
    "    def id_by_word(self, word): \n",
    "        return self.word_ids[word]\n",
    "        \n",
    "    def id_by_pos(self, pos): \n",
    "        return self.pos_ids[pos]\n",
    "        \n",
    "    def pos_by_id(self, id):\n",
    "        return self.ids_pos[id]\n",
    "        \n",
    "    def id_by_ner(self, ner): \n",
    "        return self.ner_ids[ner]\n",
    "        \n",
    "    def ner_by_id(self, id): \n",
    "        return self.ids_ner[id]\n",
    "    \n",
    "    def size_ner(self):\n",
    "        return len(self.ner_ids)\n",
    "    \n",
    "    def size_pos(self):\n",
    "        return len(self.pos_ids)\n",
    "    \n",
    "    def size_vocab(self):\n",
    "        return len(self.word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Dictionary('./resources/vocab/tag.json', './resources/bert_base_uncased_tokenizer/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger\n",
    "``base code to execute``\n",
    "* ``import spacy``\n",
    "* ``import en_core_web_sm``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    useful functions to clean up text before process it for tagging \n",
    "\"\"\"\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(): \n",
    "    def __init__(self):\n",
    "        self.__nlpTagger = en_core_web_sm.load()\n",
    "        \n",
    "    def _clean_text(self, text): \n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else : \n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "    \n",
    "    def _run_strip_accent(self, text): \n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        \n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def lowercase_text(self, t, all_special_tokens): \n",
    "        \"\"\"\n",
    "            convert text into lowercase\n",
    "        \"\"\"\n",
    "        #strip accents\n",
    "        t =  self._run_strip_accent(t)\n",
    "        \n",
    "        #clean text \n",
    "        t = self._clean_text(t)\n",
    "        \n",
    "        # convert non-special tokens to lowercase\n",
    "        escaped_special_toks = [re.escape(s_tok) for s_tok in all_special_tokens]\n",
    "        pattern = r\"(\" + r\"|\".join(escaped_special_toks) + r\")|\" + r\"(.+?)\"\n",
    "        return re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), t)\n",
    "    \n",
    "    def get_tag(self, sentence, tokens_bert, all_special_tokens):\n",
    "        \"\"\"\n",
    "            get tag of a given sentence : \n",
    "            arg :\n",
    "                - sentence \n",
    "                - tokens_bert : list of tokens provider by the tokenizer of BERT-base-uncased\n",
    "                - all_special_tokens : list of all the special tokens of the BERT tokenizer\n",
    "        \"\"\"\n",
    "        sentence = sentence.strip()\n",
    "        result = []\n",
    "        sentence = self.lowercase_text(sentence, all_special_tokens)\n",
    "        tags = self.__nlpTagger(sentence)\n",
    "        posTags = [(w.text, w.tag_, False) for w in tags]\n",
    "        nerTags = [(ent.text, ent.start_char, ent.end_char, ent.label_, False) for ent in tags.ents]\n",
    "        \n",
    "        index = 0\n",
    "        \n",
    "        for inittoken in tokens_bert:\n",
    "            flag = True\n",
    "            token = inittoken\n",
    "            if token.startswith('#'):\n",
    "                token = token[2:]\n",
    "                \n",
    "            ## handle ner tagging\n",
    "            if sentence.startswith(' '):\n",
    "                sentence = sentence.strip()\n",
    "                index += 1\n",
    "                \n",
    "            if sentence.startswith(token):\n",
    "                sentence = sentence[len(token):]\n",
    "                \n",
    "            if len(nerTags) != 0: \n",
    "                text, start, end, label, text_flag = nerTags[0]\n",
    "                if index >= start and index < end:\n",
    "                    if text_flag : \n",
    "                        ner_label = 'I-' + label\n",
    "                    else : \n",
    "                        ner_label = 'B-' + label\n",
    "                        text_flag = True\n",
    "                        nerTags[0] = (text, start, end, label, text_flag)\n",
    "                    flag = False\n",
    "\n",
    "                if text.startswith(' '):\n",
    "                    text = text.strip()\n",
    "                \n",
    "                if text.startswith(token):\n",
    "                    text = text[len(token):]\n",
    "                    if text == '':\n",
    "                        nerTags.pop(0)\n",
    "                    else : \n",
    "                        nerTags[0] = (text, start, end, label, text_flag)\n",
    "            if flag:\n",
    "                ner_label = 'O'\n",
    "            index += len(token)\n",
    "            \n",
    "            \n",
    "            if len(posTags) != 0:\n",
    "                word, pos_l, word_flag = posTags[0]\n",
    "                if word.startswith(token): \n",
    "                    word = word[len(token):]\n",
    "                    if word_flag : \n",
    "                        pos_label = 'I-' + pos_l\n",
    "                    else : \n",
    "                        pos_label = 'B-' + pos_l\n",
    "                        word_flag = True\n",
    "                        posTags[0] = (word, pos_l, word_flag)\n",
    "                        \n",
    "                    if word == '':\n",
    "                        posTags.pop(0)\n",
    "                    else :\n",
    "                        posTags[0] = (word, pos_l, word_flag)\n",
    "            result.append((inittoken, ner_label, pos_label))\n",
    "        return result\n",
    "    \n",
    "    def _insert_answer(self,input_list, sentence, tokenizer, start_answer, end_answer):\n",
    "        \"\"\"\n",
    "            insert the tag of the bouandaries answer token\n",
    "        \"\"\"\n",
    "        before_answer = tokenizer.tokenize(sentence[:start_answer])\n",
    "        answer = tokenizer.tokenize(sentence[start_answer : end_answer])\n",
    "        after_answer = tokenizer.tokenize(sentence[end_answer :])\n",
    "        \n",
    "        # init the result with the begining token\n",
    "        result = [(constant['cls'], 0, 0)]\n",
    "        \n",
    "        result += input_list[:len(before_answer)]\n",
    "        result += [(constant['start_answer_token'], 0, 0)]\n",
    "        result += input_list[len(before_answer):len(before_answer) + len(answer)]\n",
    "        result += [(constant['end_answer_token'], 0, 0)]\n",
    "        result += input_list[len(before_answer) + len(answer):]\n",
    "        \n",
    "        result += [(constant['sep'], 0, 0)]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_tag_index(self, sentence, tokenizer, dic, start_answer=None, end_answer=None) : \n",
    "        \"\"\"\n",
    "            get index in dictionary of tags of a given sentence\n",
    "        \"\"\"\n",
    "        bert_tokens = tokenizer.tokenize(sentence)\n",
    "        all_special_tokens = tokenizer.all_special_tokens\n",
    "        tags = self.get_tag(sentence, bert_tokens, all_special_tokens)\n",
    "        result = []\n",
    "        for tag in tags:\n",
    "            result.append((tag[0], dic.id_by_ner(tag[1]), dic.id_by_pos(tag[2])))\n",
    "        if start_answer and end_answer: \n",
    "            result = self._insert_answer(result, sentence, tokenizer, start_answer, end_answer)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building of dataset based on SQUAD 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(): \n",
    "    '''\n",
    "        rule of validation of data in dataset: \n",
    "            context cannot be null\n",
    "            answer a question regarding the context cannot be impossible\n",
    "    '''\n",
    "    def __init__(self, path_to_dataset): \n",
    "        self.dataset = {}\n",
    "        index = -1\n",
    "        with open(path_to_dataset) as json_file: \n",
    "            \n",
    "            data = json.load(json_file)\n",
    "            for batch_data in data['data']: \n",
    "                index += 1\n",
    "                self.dataset[index] = {}\n",
    "                for paragraph in batch_data['paragraphs']:\n",
    "                    if paragraph['context']: \n",
    "                        self.dataset[index]['context'] = paragraph['context']\n",
    "                        # loop over question and answers for a given context\n",
    "                        if len(paragraph['qas']) != 0:\n",
    "                            self.dataset[index]['qas'] = []\n",
    "                            for qas in paragraph['qas']: \n",
    "                                if qas['is_impossible'] is False: \n",
    "                                    question = qas['question']\n",
    "\n",
    "                                    # loop over answers\n",
    "                                    length_answer = -1\n",
    "                                    start_answer = 10e4\n",
    "                                    end_answer = None\n",
    "                                    for ans in qas['answers']: \n",
    "                                        if ans['answer_start'] < start_answer:\n",
    "                                            start_answer = ans['answer_start']\n",
    "                                        if length_answer <= len(ans['text']): \n",
    "                                            end_answer = start_answer + len(ans['text'])\n",
    "                                    self.dataset[index]['qas'].append((question, start_answer, end_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('./dataset/squad2.0/train-v2.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model\n",
    "* sourceEmbeddingLayer\n",
    "* PositionEncoderLayer\n",
    "* GeneratorLayer\n",
    "* Transformer\n",
    "* GreedyDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=768, path_to_bert_tokenizer = None, path_to_bert_model = None):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.d_model = d_model \n",
    "        self.bertEmb = BertEmb(path_to_bert_tokenizer, path_to_bert_model)\n",
    "        \n",
    "    def forward(self, sentence, start_answer = None, end_answer = None):\n",
    "        \n",
    "        # form of result : (encoders , padding_length): \n",
    "        #     encoders : embedding + padding representation of sentence \n",
    "        #     padding_length : length of padding sequence added\n",
    "        \n",
    "        '''\n",
    "            result : encoders_layers, index_padding \n",
    "                encoders_layers : words vector representation of sequence plus pad\n",
    "                index_padding : index at which begin padding\n",
    "        '''\n",
    "        output = self.bertEmb.get_word_emb(sentence, start_answer, end_answer)\n",
    "        output = output * math.sqrt(self.d_model)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoder(nn.Module): \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=2500):\n",
    "        super(PositionEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len,d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        #shape of x : (x.size(0), d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model=768, vocab_size =30522, path_to_bert_tokenizer=None, path_to_bert_model=None):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.transformer = Transformer(d_model=768)\n",
    "        self.embedding = EmbeddingLayer(d_model, path_to_bert_tokenizer, path_to_bert_model)\n",
    "        self.position_encoder = PositionEncoder(d_model)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        #initialize parameter of model\n",
    "        self._reset_parameters()\n",
    "        \n",
    "        \n",
    "    def forward(self, src, tgt, start_answer=None, end_answer=None):\n",
    "        src = self.embedding(src, start_answer, end_answer)\n",
    "        src = self.position_encoder(src)\n",
    "        tgt = self.embedding.bertEmb.get_word_emb_by_indices(tgt)\n",
    "        tgt = self.position_encoder(tgt)\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.shape[0])\n",
    "        memory = self.transformer.encoder(src)\n",
    "        output = self.transformer.decoder(tgt, memory, tgt_mask = tgt_mask)\n",
    "        output = self.linear(output[:, -1])\n",
    "        output = self.log_softmax(output)\n",
    "        \n",
    "        return memory, output\n",
    "    \n",
    "    def decode(self, tgt, memory):\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.shape[0])\n",
    "        output = self.transformer.decoder(tgt, memory, tgt_mask = tgt_mask)\n",
    "        output = self.linear(output[:, -1])\n",
    "        output = self.log_softmax(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kwate/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Using cache found in /home/kwate/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(context, start_answer, end_answer, question_indices, model, optimizer, criterion): \n",
    "    \"\"\"\n",
    "        assumptions :\n",
    "            - question indexed_tokens of question provided by bert_tokenizer\n",
    "    \"\"\"\n",
    "    #set grad of optimizer to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    target_length = len(question_indices)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
